{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T19:36:51.628200Z",
     "iopub.status.busy": "2020-11-07T19:36:51.627962Z",
     "iopub.status.idle": "2020-11-07T19:36:57.582621Z",
     "shell.execute_reply": "2020-11-07T19:36:57.581955Z",
     "shell.execute_reply.started": "2020-11-07T19:36:51.628170Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from util.scraping\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sleep = 8\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "headers = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T16:21:29.343950Z",
     "iopub.status.busy": "2020-11-06T16:21:29.343719Z",
     "iopub.status.idle": "2020-11-06T16:21:31.077258Z",
     "shell.execute_reply": "2020-11-06T16:21:31.076686Z",
     "shell.execute_reply.started": "2020-11-06T16:21:29.343922Z"
    }
   },
   "outputs": [],
   "source": [
    "work_id = 19979299\n",
    "url = 'http://archiveofourown.org/works/' + str(work_id) + \"?view_adult=true?view_full_work=true\"\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "chaps_html = soup.find(\"div\", id=\"chapters\").find_all(class_=\"userstuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[search url](https://archiveofourown.org/works?utf8=%E2%9C%93&work_search%5Bsort_column%5D=kudos_count&include_work_search%5Bfandom_ids%5D%5B%5D=65&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=F&work_search%5Bcomplete%5D=T&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=en&commit=Sort+and+Filter&tag_id=Avatar%3A+The+Last+Airbender\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T18:13:47.693011Z",
     "iopub.status.busy": "2020-11-06T18:13:47.692820Z",
     "iopub.status.idle": "2020-11-06T18:13:47.698632Z",
     "shell.execute_reply": "2020-11-06T18:13:47.698134Z",
     "shell.execute_reply.started": "2020-11-06T18:13:47.692990Z"
    }
   },
   "outputs": [],
   "source": [
    "# map sentiment trend of hurt-comfort fics?\n",
    "\n",
    "def chap_html_to_str(chap_html):\n",
    "    all_p = chap_html.find_all(\"p\")\n",
    "    str_arr = list(map(lambda x: str(x.text).replace(\"\\xa0\", \" \"), all_p))\n",
    "    str_arr = [st for st in str_arr if st]\n",
    "    return ' '.join(str_arr)\n",
    "\n",
    "# could select only common tags here, or just do it afterwards in cleaning\n",
    "# https://archiveofourown.org/faq/tags?language_id=en#canonicalhow \n",
    "# https://fanlore.org/wiki/AO3_Tag_Wrangling\n",
    "# try to associate tags with canonical tags\n",
    "\n",
    "the_html = None\n",
    "\n",
    "def scrape_fic(work_id, by_chapter=True):\n",
    "    print(\"getting fic\", work_id)\n",
    "    url = 'http://archiveofourown.org/works/' + str(work_id) + \"?view_adult=true?view_full_work=true\"\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # \n",
    "    \n",
    "    chaps_html = soup.find(\"div\", id=\"chapters\").find_all(class_=\"userstuff\")\n",
    "    chaps_clean = list(map(chap_html_to_str, chaps_html))\n",
    "    \n",
    "    addl_tags_html = soup.find_all(class_=\"freeform tags\")\n",
    "    if len(addl_tags_html) >= 2:\n",
    "        addl_tags_html = addl_tags_html[1].find_all(class_=\"tag\")\n",
    "        addl_tags = list(map(lambda x: x.text, addl_tags_html))\n",
    "    else:\n",
    "        addl_tags = []\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    #print(chaps_clean)\n",
    "    #print(addl_tags)\n",
    "    fic = pd.Series([chaps_clean, addl_tags], index=[\"text\", \"addl_tags\"])\n",
    "    \n",
    "    return fic # if by_chapter else \" \".join(chaps_clean)#, notes_clean, summ_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T17:44:44.549975Z",
     "iopub.status.busy": "2020-11-06T17:44:44.549789Z",
     "iopub.status.idle": "2020-11-06T17:44:44.555511Z",
     "shell.execute_reply": "2020-11-06T17:44:44.554980Z",
     "shell.execute_reply.started": "2020-11-06T17:44:44.549954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  tags\n",
       "0     1     2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1, 2], columns=[\"text\", \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T17:44:44.903346Z",
     "iopub.status.busy": "2020-11-06T17:44:44.903158Z",
     "iopub.status.idle": "2020-11-06T17:44:44.905676Z",
     "shell.execute_reply": "2020-11-06T17:44:44.905092Z",
     "shell.execute_reply.started": "2020-11-06T17:44:44.903326Z"
    }
   },
   "outputs": [],
   "source": [
    "canonical_tags = {}\n",
    "tag_map = {}\n",
    "\n",
    "# https://archiveofourown.org/tags/obligatory%20%22gaang%20finds%20out%20about%20Zuko's%20scar%22%20fic\n",
    "#def wrangle_tag(tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T18:16:13.390406Z",
     "iopub.status.busy": "2020-11-06T18:16:13.390201Z",
     "iopub.status.idle": "2020-11-06T18:16:13.401196Z",
     "shell.execute_reply": "2020-11-06T18:16:13.400619Z",
     "shell.execute_reply.started": "2020-11-06T18:16:13.390385Z"
    }
   },
   "outputs": [],
   "source": [
    "search_url = \"https://archiveofourown.org/works?utf8=%E2%9C%93&work_search%5Bsort_column%5D=kudos_count&include_work_search%5Bfandom_ids%5D%5B%5D=65&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=F&work_search%5Bcomplete%5D=T&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=en&commit=Sort+and+Filter&tag_id=Avatar%3A+The+Last+Airbender\"\n",
    "\n",
    "# single chapter only\n",
    "\n",
    "test = None\n",
    "\n",
    "def match_conditions(lang, words, chapters, single_only=False, word_range=(0,0)):\n",
    "    if lang != \"English\":\n",
    "        return False\n",
    "    if single_only and chapters != 1:\n",
    "        return False\n",
    "    if words < word_range[0] or (word_range[1] and words > word_range[1]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_id_from_heading(heading_html):\n",
    "    link = heading_html.find(\"a\")\n",
    "    if link:\n",
    "        return link['href'].split(\"/\")[2]\n",
    "    \n",
    "def parse_meta(info_html):\n",
    "    work_id = info_html.get(\"id\").split(\"_\")[1]\n",
    "    lang = info_html.find_all(class_=\"language\")[1].text\n",
    "    words = int(info_html.find_all(class_=\"words\")[1].text.replace(\",\", \"\"))\n",
    "    chapters = int(info_html.find_all(class_=\"chapters\")[1].text.split(\"/\")[0])\n",
    "    \n",
    "    return {\"work_id\": work_id, \"lang\": lang, \"words\": words, \"chapters\": chapters}\n",
    "    \n",
    "# test running out of works\n",
    "def get_work_ids(url, count, page=1, single_only=False, word_range=(0,0)):\n",
    "    print(f\"getting page {page}, {count} works left\")\n",
    "    url_page = url if page == 1 else url + \"&page=\" + str(page)\n",
    "\n",
    "    r = requests.get(url_page, headers=headers)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    info_list = soup.find_all(class_=\"blurb\")\n",
    "    \n",
    "    if not info_list:\n",
    "        return []\n",
    "    work_info = [parse_meta(info_html) for info_html in info_list]\n",
    "    work_ids = [x[\"work_id\"] for x in work_info if match_conditions(x[\"lang\"], x[\"words\"], x[\"chapters\"], single_only=single_only, word_range=word_range)]\n",
    "\n",
    "    if count <= len(work_ids):\n",
    "        return work_ids[:count]    \n",
    "    time.sleep(sleep)\n",
    "    return work_ids + get_work_ids(url, count-len(work_ids), page+1, single_only=single_only, word_range=word_range)\n",
    "\n",
    "def get_work_info(url, count, page=1, single_only=False, word_range=(0,0)):\n",
    "    print(f\"getting page {page}, {count} works left\")\n",
    "    url_page = url if page == 1 else url + \"&page=\" + str(page)\n",
    "\n",
    "    r = requests.get(url_page, headers=headers)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    info_list = soup.find_all(class_=\"blurb\")\n",
    "    \n",
    "    if not info_list:\n",
    "        return []\n",
    "    work_info = [parse_meta(info_html) for info_html in info_list]\n",
    "    work_info = [x for x in work_info if match_conditions(x[\"lang\"], x[\"words\"], x[\"chapters\"], single_only=single_only, word_range=word_range)]\n",
    "\n",
    "    if count <= len(work_info):\n",
    "        print(\"finished\")\n",
    "        return work_info[:count]    \n",
    "    time.sleep(sleep)\n",
    "    return work_info + get_work_info(url, count-len(work_info), page+1, single_only=single_only, word_range=word_range)\n",
    "\n",
    "# options: single chapter only, word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T18:15:07.300775Z",
     "iopub.status.busy": "2020-11-06T18:15:07.300584Z",
     "iopub.status.idle": "2020-11-06T18:16:07.777993Z",
     "shell.execute_reply": "2020-11-06T18:16:07.777171Z",
     "shell.execute_reply.started": "2020-11-06T18:15:07.300755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 1, 100 works left\n",
      "getting page 2, 83 works left\n",
      "getting page 3, 67 works left\n",
      "getting page 4, 54 works left\n",
      "getting page 5, 40 works left\n",
      "getting page 6, 25 works left\n",
      "getting page 7, 8 works left\n"
     ]
    }
   ],
   "source": [
    "fics_info = get_work_info(search_url, 100, single_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T18:16:18.002820Z",
     "iopub.status.busy": "2020-11-06T18:16:18.002604Z",
     "iopub.status.idle": "2020-11-06T18:16:18.006719Z",
     "shell.execute_reply": "2020-11-06T18:16:18.006012Z",
     "shell.execute_reply.started": "2020-11-06T18:16:18.002797Z"
    }
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame.from_dict(fics_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T18:16:23.565443Z",
     "iopub.status.busy": "2020-11-06T18:16:23.565259Z",
     "iopub.status.idle": "2020-11-06T18:33:47.008297Z",
     "shell.execute_reply": "2020-11-06T18:33:47.007651Z",
     "shell.execute_reply.started": "2020-11-06T18:16:23.565424Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting fic 14921627\n",
      "getting fic 15753762\n",
      "getting fic 14510448\n",
      "getting fic 21526240\n",
      "getting fic 19979299\n",
      "getting fic 1078856\n",
      "getting fic 1938096\n",
      "getting fic 10990518\n",
      "getting fic 21990778\n",
      "getting fic 21086960\n",
      "getting fic 19416352\n",
      "getting fic 1874571\n",
      "getting fic 15923450\n",
      "getting fic 330627\n",
      "getting fic 16964727\n",
      "getting fic 24400723\n",
      "getting fic 22011667\n",
      "getting fic 23953027\n",
      "getting fic 23439082\n",
      "getting fic 16621667\n",
      "getting fic 23346418\n",
      "getting fic 15925412\n",
      "getting fic 21383983\n",
      "getting fic 956643\n",
      "getting fic 21327295\n",
      "getting fic 23320117\n",
      "getting fic 24674929\n",
      "getting fic 21455932\n",
      "getting fic 1214218\n",
      "getting fic 20103208\n",
      "getting fic 10771164\n",
      "getting fic 24100726\n",
      "getting fic 19255837\n",
      "getting fic 6441391\n",
      "getting fic 20655671\n",
      "getting fic 19856629\n",
      "getting fic 24596833\n",
      "getting fic 23132179\n",
      "getting fic 22907446\n",
      "getting fic 33751\n",
      "getting fic 24560689\n",
      "getting fic 15861333\n",
      "getting fic 24056428\n",
      "getting fic 22105117\n",
      "getting fic 17385011\n",
      "getting fic 24246931\n",
      "getting fic 20014126\n",
      "getting fic 2250771\n",
      "getting fic 367066\n",
      "getting fic 21076643\n",
      "getting fic 17554397\n",
      "getting fic 208234\n",
      "getting fic 21363562\n",
      "getting fic 21895918\n",
      "getting fic 22765492\n",
      "getting fic 23538337\n",
      "getting fic 128154\n",
      "getting fic 22633993\n",
      "getting fic 21539236\n",
      "getting fic 18333971\n",
      "getting fic 16471298\n",
      "getting fic 22704988\n",
      "getting fic 1053779\n",
      "getting fic 21982333\n",
      "getting fic 22368229\n",
      "getting fic 24156103\n",
      "getting fic 21284915\n",
      "getting fic 23824927\n",
      "getting fic 349305\n",
      "getting fic 10656105\n",
      "getting fic 24972334\n",
      "getting fic 24927067\n",
      "getting fic 16220267\n",
      "getting fic 21426649\n",
      "getting fic 24752242\n",
      "getting fic 22050091\n",
      "getting fic 7444282\n",
      "getting fic 10762515\n",
      "getting fic 24949420\n",
      "getting fic 24472705\n",
      "getting fic 23705629\n",
      "getting fic 21879184\n",
      "getting fic 23978929\n",
      "getting fic 22192588\n",
      "getting fic 17307311\n",
      "getting fic 24710086\n",
      "getting fic 19424806\n",
      "getting fic 24268210\n",
      "getting fic 20432777\n",
      "getting fic 24326725\n",
      "getting fic 830028\n",
      "getting fic 21600172\n",
      "getting fic 22328608\n",
      "getting fic 25039363\n",
      "getting fic 20777783\n",
      "getting fic 19739527\n",
      "getting fic 25231696\n",
      "getting fic 21882595\n",
      "getting fic 24934321\n",
      "getting fic 22213618\n"
     ]
    }
   ],
   "source": [
    "text_df = pd.DataFrame(info_df[\"work_id\"].apply(scrape_fic))\n",
    "final_df = pd.concat([info_df, text_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T19:12:03.660073Z",
     "iopub.status.busy": "2020-11-06T19:12:03.659881Z",
     "iopub.status.idle": "2020-11-06T19:12:03.669253Z",
     "shell.execute_reply": "2020-11-06T19:12:03.668621Z",
     "shell.execute_reply.started": "2020-11-06T19:12:03.660053Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.to_pickle(\"avatar_fic_100_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-06T17:45:17.962960Z",
     "iopub.status.idle": "2020-11-06T17:45:17.963204Z"
    }
   },
   "outputs": [],
   "source": [
    "info_dict[\"work_id\"].apply(scrape_fic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dict = pd.DataFrame(info_dict[\"work_id\"].ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for info in single_chapter_fics: #avatar_single_chap:\n",
    "    texts.append(scrape_fic(info[\"work_id\"]))\n",
    "    #try:\n",
    "    #    texts.append(scrape_fic(id))\n",
    "    #except Exception as e:\n",
    "    #    print(f\"error with work {id}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:33:31.166674Z",
     "iopub.status.busy": "2020-11-05T20:33:31.166443Z",
     "iopub.status.idle": "2020-11-05T20:33:31.211629Z",
     "shell.execute_reply": "2020-11-05T20:33:31.210844Z",
     "shell.execute_reply.started": "2020-11-05T20:33:31.166649Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(texts)\n",
    "df.to_csv(\"texts_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:34:51.895141Z",
     "iopub.status.busy": "2020-11-05T20:34:51.894927Z",
     "iopub.status.idle": "2020-11-05T20:34:51.897472Z",
     "shell.execute_reply": "2020-11-05T20:34:51.896947Z",
     "shell.execute_reply.started": "2020-11-05T20:34:51.895118Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.to_pickle(\"50_texts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_fic(21526240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T18:59:24.715128Z",
     "iopub.status.busy": "2020-11-05T18:59:24.714912Z",
     "iopub.status.idle": "2020-11-05T18:59:24.722488Z",
     "shell.execute_reply": "2020-11-05T18:59:24.721512Z",
     "shell.execute_reply.started": "2020-11-05T18:59:24.715105Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-25aeab3d0adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOut\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 22"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "chaps_html = soup.find(\"div\", id=\"chapters\").find_all(class_=\"userstuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T19:14:44.799948Z",
     "iopub.status.busy": "2020-11-05T19:14:44.799762Z",
     "iopub.status.idle": "2020-11-05T19:14:48.915966Z",
     "shell.execute_reply": "2020-11-05T19:14:48.915465Z",
     "shell.execute_reply.started": "2020-11-05T19:14:44.799929Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T16:21:21.046290Z",
     "iopub.status.busy": "2020-11-06T16:21:21.046102Z",
     "iopub.status.idle": "2020-11-06T16:21:21.050374Z",
     "shell.execute_reply": "2020-11-06T16:21:21.049298Z",
     "shell.execute_reply.started": "2020-11-06T16:21:21.046270Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-8900984dca93>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8900984dca93>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    str_arr = list(map(lambda x: str(x.text).replace(\"\\xa0\", \"\").replace(\"\\n\", \"\").replace(\"\\\", \"\"), all_p))\u001b[0m\n\u001b[0m                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "chap_html = chaps_html[0]\n",
    "all_p = chap_html.find_all(\"p\")\n",
    "str_arr = list(map(lambda x: str(x.text).replace(\"\\xa0\", \"\").replace(\"\\n\", \"\").replace(\"\\\", \"\"), all_p))\n",
    "str_arr = [st for st in str_arr if st]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
