{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T20:57:38.795203Z",
     "iopub.status.busy": "2020-11-04T20:57:38.795023Z",
     "iopub.status.idle": "2020-11-04T20:57:39.214886Z",
     "shell.execute_reply": "2020-11-04T20:57:39.214320Z",
     "shell.execute_reply.started": "2020-11-04T20:57:38.795183Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sleep = 8\n",
    "\n",
    "# map sentiment trend of hurt-comfort fics?\n",
    "\n",
    "test = None\n",
    "\n",
    "def chap_html_to_str(chap_html):\n",
    "    #global test\n",
    "    all_p = chap_html.find_all(\"p\")\n",
    "    #test = all_p\n",
    "    str_arr = list(map(lambda x: str(x.string).replace(\"\\xa0\", \"\"), all_p))\n",
    "    str_arr = [st for st in str_arr if st]\n",
    "    return ' '.join(str_arr)\n",
    "\n",
    "def scrape_fic(work_id, by_chapter = True):\n",
    "    print(\"getting fic\", work_id)\n",
    "    url = 'http://archiveofourown.org/works/' + str(work_id) + \"?view_full_work=true?view_adult=true\"\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    chaps_html = soup.find(\"div\", id=\"chapters\").find_all(class_=\"userstuff\")\n",
    "   \n",
    "    chaps_clean = list(map(chap_html_to_str, chaps_html))\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    return chaps_clean # if by_chapter else \" \".join(chaps_clean)#, notes_clean, summ_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[search url](https://archiveofourown.org/works?utf8=%E2%9C%93&work_search%5Bsort_column%5D=kudos_count&include_work_search%5Bfandom_ids%5D%5B%5D=65&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=F&work_search%5Bcomplete%5D=T&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=en&commit=Sort+and+Filter&tag_id=Avatar%3A+The+Last+Airbender\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T20:57:54.795845Z",
     "iopub.status.busy": "2020-11-04T20:57:54.795617Z",
     "iopub.status.idle": "2020-11-04T20:57:57.073018Z",
     "shell.execute_reply": "2020-11-04T20:57:57.072436Z",
     "shell.execute_reply.started": "2020-11-04T20:57:54.795820Z"
    }
   },
   "outputs": [],
   "source": [
    "search_url = \"https://archiveofourown.org/works?utf8=%E2%9C%93&work_search%5Bsort_column%5D=kudos_count&include_work_search%5Bfandom_ids%5D%5B%5D=65&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=F&work_search%5Bcomplete%5D=T&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=en&commit=Sort+and+Filter&tag_id=Avatar%3A+The+Last+Airbender\"\n",
    "\n",
    "# single chapter only\n",
    "\n",
    "test = None\n",
    "\n",
    "def get_id_from_heading(heading_html):\n",
    "    link = heading_html.find(\"a\")\n",
    "    if link:\n",
    "        return link['href'].split(\"/\")[2]\n",
    "    \n",
    "def parse_meta(meta_html):\n",
    "    work_id = meta_html.get(\"id\").split(\"_\")[1]\n",
    "    lang = meta_html.find_all(class_=\"language\")[1].text\n",
    "    words = int(meta_html.find_all(class_=\"words\")[1].text.replace(\",\", \"\"))\n",
    "    chapters = int(meta_html.find_all(class_=\"chapters\")[1].text.split(\"/\")[0])\n",
    "    \n",
    "    return work_id, lang, words, chapters\n",
    "\n",
    "def get_ids(url, count, page=1, single_only=False):\n",
    "    global test\n",
    "    # what if we run out?\n",
    "    \n",
    "    url_page = url if page == 1 else url + \"&page=\" + str(page)\n",
    "    \n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    meta_list = soup.find_all(class_=\"blurb\")\n",
    "    \n",
    "    meta_parsed = [parse_meta(meta_html) for meta_html in meta_list]\n",
    "\n",
    "    #headings = soup.find_all(\"h4\", class_=\"heading\")\n",
    "    \n",
    "    #ids = list(map(get_id_from_heading, headings))\n",
    "    #ids = [id for id in ids if id]\n",
    "    \n",
    "    ids = [x[0] for x in meta_parsed if x[1] == \"English\" and (True if not single_only else False or x[3] == 1)]\n",
    "    \n",
    "    if count < 20:\n",
    "        return ids[:count]    \n",
    "    return ids + get_ids(url, count-20, page+1)\n",
    "    \n",
    "ids = get_ids(search_url, 20, single_only=True)\n",
    "\n",
    "# options: single chapter only, word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T20:57:57.074496Z",
     "iopub.status.busy": "2020-11-04T20:57:57.074168Z",
     "iopub.status.idle": "2020-11-04T20:57:57.082320Z",
     "shell.execute_reply": "2020-11-04T20:57:57.081853Z",
     "shell.execute_reply.started": "2020-11-04T20:57:57.074436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14921627',\n",
       " '15753762',\n",
       " '14510448',\n",
       " '21526240',\n",
       " '19979299',\n",
       " '1078856',\n",
       " '1938096',\n",
       " '10990518',\n",
       " '21990778',\n",
       " '21086960',\n",
       " '19416352',\n",
       " '1874571',\n",
       " '15923450',\n",
       " '330627',\n",
       " '16964727',\n",
       " '24400723',\n",
       " '22011667']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T18:41:40.890159Z",
     "iopub.status.busy": "2020-11-04T18:41:40.889948Z",
     "iopub.status.idle": "2020-11-04T18:43:34.314580Z",
     "shell.execute_reply": "2020-11-04T18:43:34.313526Z",
     "shell.execute_reply.started": "2020-11-04T18:41:40.890137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting fic 14921627\n",
      "getting fic 15753762\n",
      "getting fic 14510448\n",
      "getting fic 21526240\n",
      "getting fic 19979299\n",
      "getting fic 1078856\n",
      "getting fic 1938096\n",
      "getting fic 10990518\n",
      "getting fic 21990778\n",
      "getting fic 21086960\n",
      "getting fic 19416352\n",
      "getting fic 1874571\n",
      "getting fic 15923450\n",
      "getting fic 330627\n",
      "getting fic 16964727\n",
      "getting fic 24400723\n",
      "getting fic 22011667\n"
     ]
    }
   ],
   "source": [
    "texts = [scrape_fic(id) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
