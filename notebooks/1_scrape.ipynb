{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses routines from [```scrape.py```](../util/scrape.py) to scrape fanfiction from [Archive of our Own](http://archiveofourown.org) (AO3). We've chosen to focus on <i>Avatar: The Last Airbender</i> (which we'll call Avatar) fanfiction for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T17:46:52.686581Z",
     "iopub.status.busy": "2020-11-15T17:46:52.686277Z",
     "iopub.status.idle": "2020-11-15T17:46:52.721972Z",
     "shell.execute_reply": "2020-11-15T17:46:52.721355Z",
     "shell.execute_reply.started": "2020-11-15T17:46:52.686558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from util.scrape import get_search_url, get_works_info, scrape_fic_list\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our search query: completed Avatar fanfiction (\"fics\") with at least 400 kudos. We restrict ourselves to English-language works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T15:29:27.116270Z",
     "iopub.status.busy": "2020-11-15T15:29:27.116043Z",
     "iopub.status.idle": "2020-11-15T15:29:27.139058Z",
     "shell.execute_reply": "2020-11-15T15:29:27.138389Z",
     "shell.execute_reply.started": "2020-11-15T15:29:27.116245Z"
    }
   },
   "outputs": [],
   "source": [
    "avatar_search = get_search_url(\"Avatar: The Last Airbender\", min_kudos=400, complete=True, english_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape the search page to collect metadata for fics we would like to include in our corpus. Here we restrict our collection to works that are not in a series (some authors post dozens of chapters as individual fics, which biases our data towards those few authors) and are longer than 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T15:29:32.966482Z",
     "iopub.status.busy": "2020-11-15T15:29:32.966260Z",
     "iopub.status.idle": "2020-11-15T15:42:06.303050Z",
     "shell.execute_reply": "2020-11-15T15:42:06.302485Z",
     "shell.execute_reply.started": "2020-11-15T15:29:32.966458Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358 potential matches found\n",
      "getting page 1, 2000 works left\n",
      "getting page 6, 1957 works left\n",
      "getting page 11, 1904 works left\n",
      "getting page 16, 1864 works left\n",
      "getting page 21, 1828 works left\n",
      "getting page 26, 1782 works left\n",
      "getting page 31, 1730 works left\n",
      "getting page 36, 1682 works left\n",
      "getting page 41, 1630 works left\n",
      "getting page 46, 1580 works left\n",
      "getting page 51, 1531 works left\n",
      "getting page 56, 1487 works left\n",
      "getting page 61, 1451 works left\n",
      "getting page 66, 1412 works left\n",
      "getting page 71, 1385 works left\n",
      "getting page 76, 1343 works left\n",
      "getting page 81, 1301 works left\n",
      "getting page 86, 1260 works left\n",
      "getting page 91, 1200 works left\n",
      "getting page 96, 1156 works left\n",
      "getting page 101, 1114 works left\n",
      "getting page 106, 1062 works left\n",
      "getting page 111, 1012 works left\n",
      "getting page 116, 967 works left\n",
      "ran out of fics with 937 left\n"
     ]
    }
   ],
   "source": [
    "works_info = get_works_info(avatar_search, 2000, word_range=(1000,0), exclude_series=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the metadata into our scraping function to get a dataframe with the works' texts and relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T15:50:29.653195Z",
     "iopub.status.busy": "2020-11-15T15:50:29.652968Z",
     "iopub.status.idle": "2020-11-15T17:46:52.685229Z",
     "shell.execute_reply": "2020-11-15T17:46:52.684645Z",
     "shell.execute_reply.started": "2020-11-15T15:50:29.653171Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning scrape...\n",
      "scraping fic 1/1063\n",
      "scraping fic 31/1063\n",
      "scraping fic 61/1063\n",
      "scraping fic 91/1063\n",
      "scraping fic 121/1063\n",
      "scraping fic 151/1063\n",
      "scraping fic 181/1063\n",
      "scraping fic 211/1063\n",
      "scraping fic 241/1063\n",
      "scraping fic 271/1063\n",
      "scraping fic 301/1063\n",
      "scraping fic 331/1063\n",
      "scraping fic 361/1063\n",
      "scraping fic 391/1063\n",
      "scraping fic 421/1063\n",
      "scraping fic 451/1063\n",
      "scraping fic 481/1063\n",
      "scraping fic 511/1063\n",
      "scraping fic 541/1063\n",
      "scraping fic 571/1063\n",
      "scraping fic 601/1063\n",
      "scraping fic 631/1063\n",
      "scraping fic 661/1063\n",
      "scraping fic 691/1063\n",
      "scraping fic 721/1063\n",
      "scraping fic 751/1063\n",
      "scraping fic 781/1063\n",
      "scraping fic 811/1063\n",
      "scraping fic 841/1063\n",
      "scraping fic 871/1063\n",
      "scraping fic 901/1063\n",
      "scraping fic 931/1063\n",
      "scraping fic 961/1063\n",
      "scraping fic 991/1063\n",
      "scraping fic 1021/1063\n",
      "scraping fic 1051/1063\n"
     ]
    }
   ],
   "source": [
    "fic_df = scrape_fic_list(works_info, print_every=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T17:53:04.970469Z",
     "iopub.status.busy": "2020-11-15T17:53:04.970242Z",
     "iopub.status.idle": "2020-11-15T17:53:04.993019Z",
     "shell.execute_reply": "2020-11-15T17:53:04.992352Z",
     "shell.execute_reply.started": "2020-11-15T17:53:04.970444Z"
    }
   },
   "outputs": [],
   "source": [
    "#fic_df.to_pickle(\"../data/avatar_fics_scraped.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential future work: don't put too many works from a single author in the corpus (especially if they're very short)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
