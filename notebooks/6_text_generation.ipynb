{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling & Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T14:10:31.107027Z",
     "iopub.status.busy": "2020-12-10T14:10:31.106836Z",
     "iopub.status.idle": "2020-12-10T14:10:31.112245Z",
     "shell.execute_reply": "2020-12-10T14:10:31.111656Z",
     "shell.execute_reply.started": "2020-12-10T14:10:31.107008Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, default_data_collator\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util.generate import FicDataset, EarlyStoppingCallback, clean_text_gen, count_custom_tokens, chunk_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HuggingFace and PyTorch, we construct a language model by fine-tuning the pretrained GPT-2 model on our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform some basic cleaning of extraneous characters and extra whitespace before training our GPT-2 text generation model on our corpus. We don't need to do more complex processing such as lemmatization and stopword removal as GPT-2 trains on the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.270464Z",
     "iopub.status.idle": "2020-12-10T05:37:56.270695Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/avatar_fics_processed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.272052Z",
     "iopub.status.idle": "2020-12-10T05:37:56.272622Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"cleaned\"] = df[\"text\"].map(clean_text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.273776Z",
     "iopub.status.idle": "2020-12-10T05:37:56.274185Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_pickle(\"avatar_fics_cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Add Custom Tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find and add the most common custom tokens in our corpus to the pretrained GPT-2 tokenizer. We do so by tokenizing the text some other way (we simply split by spaces) and then comparing the most common resulting tokens to GPT-2's original vocabulary.\n",
    "\n",
    "A few notes: the GPT-2 tokenizer treats capitalized words differently, and words that begin sentences differently. We'll account for the former behavior by adding some common capitalized words into our corpus that don't appear in the original vocabulary, e.g. \"Avatar.\" We have to handle the latter in our custom token search in order to properly compare words in our corpus to the pretrained vocabulary, but we don't add custom tokens for this behavior at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pretrained tokenizer and its vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.275203Z",
     "iopub.status.idle": "2020-12-10T05:37:56.275453Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some stopwords from SpaCy and NLTK and ignore those for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.276224Z",
     "iopub.status.idle": "2020-12-10T05:37:56.276418Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "stopwords_nltk = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords_spacy = nlp.Defaults.stop_words\n",
    "stopwords = stopwords_nltk.union(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.277382Z",
     "iopub.status.idle": "2020-12-10T05:37:56.277711Z"
    }
   },
   "outputs": [],
   "source": [
    "count = count_custom_tokens(df[\"cleaned\"], vocab, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.278536Z",
     "iopub.status.idle": "2020-12-10T05:37:56.278778Z"
    }
   },
   "outputs": [],
   "source": [
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list we grab the following common custom tokens. We also add a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.279558Z",
     "iopub.status.idle": "2020-12-10T05:37:56.279811Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_tokens([\"Zuko\", \"Sokka\", \"Mai\", \"Appa\", \"Katara\", \"Kya\", \"Suki\", \"Iroh\", \"Aang\", \"Toph\", \"Beifong\", \"Agni\", \"Kai\", \"Hakoda\", \"Ozai\", \"Azula\", \"Ursa\"])\n",
    "tokenizer.add_tokens([\"Avatar\", \"Tribe\", \"Uncle\", \"Tea\", \"Kingdom\", \"Air\", \"Water\", \"Earth\", \"Prince\", \"Fire\", \"Lord\", \"Nephew\", \"Temple\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.280593Z",
     "iopub.status.idle": "2020-12-10T05:37:56.280861Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"../models/tokenizer_textgen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Our Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GPT-2 takes a limited number of tokens as input (maximum 1024), we split our corpus into smaller chunks before training. The size of chunks also influences batch size, which may be important depending on our GPU memory. We choose 500 as a reasonable guess.\n",
    "\n",
    "We make sure to cut off chunks at sentence boundaries so GPT-2 doesn't train on documents with partial sentences. We also exclude overly long sentences should they occur in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.281802Z",
     "iopub.status.idle": "2020-12-10T05:37:56.282011Z"
    }
   },
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "chunked_docs = chunk_docs(df[\"cleaned\"], tokenizer, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.283418Z",
     "iopub.status.idle": "2020-12-10T05:37:56.283658Z"
    }
   },
   "outputs": [],
   "source": [
    "chunked_df = pd.DataFrame(chunked_docs, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T03:17:05.542105Z",
     "iopub.status.busy": "2020-12-10T03:17:05.541882Z",
     "iopub.status.idle": "2020-12-10T03:17:05.599590Z",
     "shell.execute_reply": "2020-12-10T03:17:05.598892Z",
     "shell.execute_reply.started": "2020-12-10T03:17:05.542083Z"
    }
   },
   "outputs": [],
   "source": [
    "chunked_df.to_pickle(\"../data/chunked_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_df = pd.read_pickle(\"../data/chunked_df.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split and PyTorch Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a train-val-test split and encode the data so it can be properly processed by HugginFace and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.284698Z",
     "iopub.status.idle": "2020-12-10T05:37:56.284941Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valtest = train_test_split(chunked_df[\"text\"], test_size = 0.2, random_state=0)\n",
    "val, test = train_test_split(valtest, test_size = 0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.285998Z",
     "iopub.status.idle": "2020-12-10T05:37:56.286238Z"
    }
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train.tolist(), truncation=True, max_length=max_tokens, padding=\"longest\")\n",
    "val_encodings = tokenizer(val.tolist(), truncation=True, max_length=max_tokens, padding=\"longest\")\n",
    "test_encodings = tokenizer(test.tolist(), truncation=True, max_length=max_tokens, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.287135Z",
     "iopub.status.idle": "2020-12-10T05:37:56.287374Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = FicDataset(train_encodings)\n",
    "val_dataset = FicDataset(val_encodings)\n",
    "test_dataset = FicDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.288205Z",
     "iopub.status.idle": "2020-12-10T05:37:56.288451Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(train_dataset, \"../data/datasets/train.data\")\n",
    "torch.save(val_dataset, \"../data/datasets/val.data\")\n",
    "torch.save(test_dataset, \"../data/datasets/test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tune our model, using early stopping to pause when validation fails to increase 3 consecutive times. Our model ran for approximately 1.5 epochs before it stopped early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.289165Z",
     "iopub.status.idle": "2020-12-10T05:37:56.289380Z"
    }
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.290196Z",
     "iopub.status.idle": "2020-12-10T05:37:56.290467Z"
    }
   },
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/checkpoints',          # output directory\n",
    "    overwrite_output_dir = True,\n",
    "    save_total_limit = 3,\n",
    "    num_train_epochs = 5,              # total # of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    save_steps=500,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='../models/logs',            # directory for storing logs\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# our util class copied from committed but not live huggingface cold\n",
    "callback = EarlyStoppingCallback()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = default_data_collator,\n",
    "    \n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-12-10T05:37:56.291301Z",
     "iopub.status.idle": "2020-12-10T05:37:56.291529Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate text after setting various parameters for the text generation sampling. A tutorial may be found [here](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:03:42.689217Z",
     "iopub.status.busy": "2020-12-10T05:03:42.688960Z",
     "iopub.status.idle": "2020-12-10T05:03:42.693428Z",
     "shell.execute_reply": "2020-12-10T05:03:42.692487Z",
     "shell.execute_reply.started": "2020-12-10T05:03:42.689189Z"
    }
   },
   "source": [
    "If running inference on a GPU, the ```cuda()``` calls may be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:02:54.405321Z",
     "iopub.status.busy": "2020-12-10T05:02:54.405096Z",
     "iopub.status.idle": "2020-12-10T05:02:56.805968Z",
     "shell.execute_reply": "2020-12-10T05:02:56.805112Z",
     "shell.execute_reply.started": "2020-12-10T05:02:54.405297Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"../models/tokenizer_textgen\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"../models/model_textgen\")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:02:56.807950Z",
     "iopub.status.busy": "2020-12-10T05:02:56.807657Z",
     "iopub.status.idle": "2020-12-10T05:02:56.813813Z",
     "shell.execute_reply": "2020-12-10T05:02:56.813096Z",
     "shell.execute_reply.started": "2020-12-10T05:02:56.807916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb48844fb0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:02:56.831121Z",
     "iopub.status.busy": "2020-12-10T05:02:56.830895Z",
     "iopub.status.idle": "2020-12-10T05:02:56.834705Z",
     "shell.execute_reply": "2020-12-10T05:02:56.833962Z",
     "shell.execute_reply.started": "2020-12-10T05:02:56.831097Z"
    }
   },
   "outputs": [],
   "source": [
    "output_length = 200\n",
    "temperature = 0.8\n",
    "top_p = 0.94\n",
    "top_k = 60\n",
    "rep_pen = 1.2\n",
    "num_return = 3\n",
    "\n",
    "context = \"Sokka \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:02:57.420398Z",
     "iopub.status.busy": "2020-12-10T05:02:57.420182Z",
     "iopub.status.idle": "2020-12-10T05:03:14.097624Z",
     "shell.execute_reply": "2020-12-10T05:03:14.097071Z",
     "shell.execute_reply.started": "2020-12-10T05:02:57.420376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")#.cuda()\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=output_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    repetition_penalty=rep_pen,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=num_return\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T05:03:14.098931Z",
     "iopub.status.busy": "2020-12-10T05:03:14.098758Z",
     "iopub.status.idle": "2020-12-10T05:03:14.115532Z",
     "shell.execute_reply": "2020-12-10T05:03:14.114957Z",
     "shell.execute_reply.started": "2020-12-10T05:03:14.098911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Sokka's lips parted. \"I can't believe I'm getting married to someone who is going after my own children,\" he said, and he sounded so sad. Zuko brought a hand up to his mouth as Zuko sighed heavily before turning to look at him again. \"You're kidding me!  \"What does that mean?  \"I can't imagine having to do it alone for the rest of your life,\" Sokka said honestly. Zuko didn't want to admit that. They'd already been through this together and had both gotten along well enough that he thought they were going to be good people eventually in their lives. \"No, I think you don't have to deal with this right now,\" Toph said. She was wearing her new boyfriend's outfit all day. It was probably just her imagination that had been running through her head when they got there, but she could see the way his expression softened when he turned away from them, back towards the tree trunk. \"Do you\n",
      "1: Sokka's mouth twitches into a smile, and Sokka shakes his head at the sight. \"Not that I'm surprised you did,\" he says, after a moment. \"I was just curious about what you were doing in Ba Sing Se. I mean, you are an interesting person. But it is also something we have been spending some time together because of, well, our mother's death -\" His eyes narrow once more, in concentration to make out a little figure in the darkness around them, then open with a chuckle. \"I am pretty sure it doesn't matter,\" he explains quickly as he looks away from the dark figure, and then back up again. \"Did you ever think about what that would be like?  \"Only for the most part. We have had a lot going on between us. And now, you know, we're in this together. I hope that means something-\" \"You said that already! Sokka laughs a bit, and then rolls his eyes\n",
      "2: Sokka, Sokka? \"Hey, I just wanted to say that my mom is dead.  He says without thinking of it any longer. \"And you've said so many things about me since then I don't think she's going anywhere,\" Zuko says honestly. Sokka looks away from him again for a second and nods sagely in return before walking away. \"You know the rest, my brother! Sokka points out with an enthusiastic smile as they ride off the car with Toph and Katara followed by a couple of other people, and she seems extremely excited for them all. Katara laughs once and she turns back towards the street, where they were once more and watches the car pull away from the curb and into the quiet parking lot. She walks away from the car and sits down on the dirt bench beside it. She looks at the phone in her hand, and she sighs against it. She has no idea if Katara knows or not what they're texting over. \"I\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(output_sequences):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
